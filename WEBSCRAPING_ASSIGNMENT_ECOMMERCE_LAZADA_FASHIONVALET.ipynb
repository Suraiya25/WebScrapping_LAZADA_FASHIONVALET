{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAZADA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import libraries\n",
    "\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "#generate url for product\n",
    "\n",
    "def get_url(product_name):\n",
    "    product_name = product_name.replace(' ','+')\n",
    "    template = 'https://www.lazada.com.my/catalog/?q={}&_keyori=ss&from=input&spm=a2o4k.home.search.go.'\n",
    "    url = template.format(product_name)\n",
    "    return url\n",
    "\n",
    "\n",
    "#create a function which will fetch the product information\n",
    "\n",
    "def get_all_products(card):   \n",
    "    pImg = card.find('img','index__image___1YObI')\n",
    "    product_image = pImg['src']\n",
    "    product_name = card.find('div','GridItem__title___8JShU').text.strip()\n",
    "    product_name = product_name.encode('ascii','ignore')\n",
    "    product_name = str(product_name,'utf-8').strip()\n",
    "    product_price = card.find('span','index__currency___Q78Jz').text.strip()\n",
    "    anchor_tag = card.a.get('href')\n",
    "    product_buy_link = 'https://www.lazada.com.my/' + anchor_tag        \n",
    "    \n",
    "    product_info = (product_image,  product_name, product_price, product_buy_link )\n",
    "    \n",
    "    return product_info\n",
    "\n",
    "def main(product):\n",
    "    records = [] #empty list, which the informmation will be put in this list\n",
    "    url = get_url(product)\n",
    "    \n",
    "    driver = webdriver.Chrome(executable_path='D:\\OneDrive - International Islamic University Malaysia\\PANDIT\\Web Scrapping Project\\\\chromedriver.exe')\n",
    "    driver.get(url)\n",
    "    driver.maximize_window()\n",
    "    time.sleep(5)\n",
    "    \n",
    "    #Define an initial value\n",
    "    temp_height=0\n",
    " \n",
    "    while True:\n",
    "        driver.execute_script(\"window.scrollBy(0,1000)\")\n",
    "        time.sleep(5)\n",
    "        check_height = driver.execute_script(\"return document.documentElement.scrollTop || window.pageYOffset || document.body.scrollTop;\")\n",
    "        if check_height==temp_height:\n",
    "            break\n",
    "        temp_height=check_height\n",
    "    \n",
    "    time.sleep(10)\n",
    "    \n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    \n",
    "    product_cards = soup.find_all('div','index__gridItem___3VkVO')\n",
    "    \n",
    "    for everyProduct in product_cards:\n",
    "        productDetails = get_all_products(everyProduct)\n",
    "        records.append(productDetails)\n",
    "        \n",
    "    \n",
    "    #Here We Are using Pandas DataFrame To Save Products Information In A CSV File\n",
    "    \n",
    "    col = ['Product_Image', 'Product_Name', 'Product_Price', 'Product_Buy_Link']\n",
    "    \n",
    "    shopee_data = pd.DataFrame(records, columns= col)\n",
    "    \n",
    "    shopee_data.to_csv('D:\\Lazadafacemask.csv')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "main('face mask')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FASHIONVALET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import libraries\n",
    "\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "#generate url for product\n",
    "\n",
    "def get_url(product_name):\n",
    "    product_name = product_name.replace(' ','+')\n",
    "    template = 'https://www.fashionvalet.com/catalogsearch/result/?q={}'\n",
    "    url = template.format(product_name)\n",
    "    return url\n",
    "\n",
    "\n",
    "#create a function which will fetch the product information\n",
    "\n",
    "def get_all_products(card):   \n",
    "    pImg = card.find(\"p\",\"fvPLPProductImage\")\n",
    "    product_image = pImg(\"img\")[0][\"src\"]\n",
    "    product_name = card.find('h3').text.strip()\n",
    "    product_price = card.find('p',class_='fvPLPProductPrice').text.strip()\n",
    "    anchor_tag = card.find('h3')\n",
    "    product_buy_link = anchor_tag.a.get('href')       \n",
    "    \n",
    "    product_info = (product_image,  product_name, product_price, product_buy_link )\n",
    "    \n",
    "    return product_info\n",
    "\n",
    "def main(product):\n",
    "    records = [] #empty list, which the informmation will be put in this list\n",
    "    url = get_url(product)\n",
    "    \n",
    "    driver = webdriver.Chrome(executable_path='D:\\OneDrive - International Islamic University Malaysia\\PANDIT\\Web Scrapping Project\\\\chromedriver.exe')\n",
    "    driver.get(url)\n",
    "    driver.maximize_window()\n",
    "    time.sleep(3)\n",
    "    #after 2 seconds thee button will be clicked\n",
    "    #after 2 seconds thee button will be clicked\n",
    "    btn = driver.find_element_by_xpath('/html/body/main/div/header/div[5]/div[1]/div[1]/div/i')\n",
    "    btn.click()\n",
    "    time.sleep(5)\n",
    "    \n",
    "    #Define an initial value\n",
    "    temp_height=0\n",
    " \n",
    "    while True:\n",
    "        driver.execute_script(\"window.scrollBy(0,1000)\")\n",
    "        time.sleep(5)\n",
    "        check_height = driver.execute_script(\"return document.documentElement.scrollTop || window.pageYOffset || document.body.scrollTop;\")\n",
    "        if check_height==temp_height:\n",
    "            break\n",
    "        temp_height=check_height\n",
    "    \n",
    "    time.sleep(10)\n",
    "    \n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "     \n",
    "    #PRODUCT CARD\n",
    "\n",
    "    product_cards = soup.find('ol','fvPLPProducts')\n",
    "\n",
    "    list_cards = []\n",
    "    product = []\n",
    "\n",
    "    for tag in product_cards('li'):\n",
    "        product = tag\n",
    "        list_cards.append(product)\n",
    "        \n",
    "    for everyProduct in list_cards:\n",
    "        productDetails = get_all_products(everyProduct)\n",
    "        records.append(productDetails)\n",
    "    \n",
    "    #Here We Are using Pandas DataFrame To Save Products Information In A CSV File\n",
    "    \n",
    "    col = ['Product_Image', 'Product_Name', 'Product_Price', 'Product_Buy_Link']\n",
    "    \n",
    "    shopee_data = pd.DataFrame(records, columns= col)\n",
    "    \n",
    "    shopee_data.to_csv('D:\\FashionValet.csv')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "main('baju kurung')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
